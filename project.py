# -*- coding: utf-8 -*-
"""BigDataProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zjDHr46iZmncx4cCypTm05B9xjLtvAJ-

# **E22CSEU0337 Arushi Garg**
"""

from pyspark.sql import SparkSession
from pyspark.sql.functions import col
from pyspark.sql.types import DoubleType, IntegerType, StringType

# Initialize Spark Session
spark = SparkSession.builder.appName("HeartDiseasePrediction").getOrCreate()

# Load the dataset
file_path = "heart24.csv"
data = spark.read.csv(file_path, header=True, inferSchema=True)

# Display the first few rows of the dataset
data.show()

data.printSchema()

# Check for missing values in each column
from pyspark.sql.functions import col, isnan, when, count

# Count missing or null values in each column
data.select([count(when(col(c).isNull() | isnan(c), c)).alias(c) for c in data.columns]).show()

from pyspark.ml.feature import StringIndexer

# Convert categorical columns to numerical indices
categorical_cols = ["cp", "restecg", "slope", "thal"]
indexers = [StringIndexer(inputCol=col, outputCol=f"{col}_indexed").fit(data) for col in categorical_cols]

# Apply the transformations
for indexer in indexers:
    data = indexer.transform(data)

# Drop original categorical columns
data = data.drop(*categorical_cols)

# Display updated dataset
data.show()

from pyspark.ml.feature import MinMaxScaler, VectorAssembler

# Continuous columns to scale
continuous_cols = ["age", "trestbps", "chol", "thalach", "oldpeak"]

# Assemble continuous features into a single vector
assembler = VectorAssembler(inputCols=continuous_cols, outputCol="continuous_features")
data = assembler.transform(data)

# Apply Min-Max Scaling
scaler = MinMaxScaler(inputCol="continuous_features", outputCol="scaled_features")
scaler_model = scaler.fit(data)
data = scaler_model.transform(data)

# Drop unnecessary columns
data = data.drop("continuous_features")
data.show()

# Split dataset into training and testing sets
train_data, test_data = data.randomSplit([0.7, 0.3], seed=42)

# Display counts
print(f"Training Data Count: {train_data.count()}")
print(f"Test Data Count: {test_data.count()}")

from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.feature import VectorAssembler
from pyspark.sql.functions import col

# Assemble features into a feature vector
feature_columns = [col for col in data.columns if col not in ['target']]  # Exclude target column
assembler = VectorAssembler(inputCols=feature_columns, outputCol="features")
df_assembled = assembler.transform(data)

# Split data into train and test sets
train_data, test_data = df_assembled.randomSplit([0.7, 0.3], seed=123)

# Initialize and train the Logistic Regression model
lr = LogisticRegression(featuresCol="features", labelCol="target")
lr_model = lr.fit(train_data)
lr_predictions = lr_model.transform(test_data)

# Evaluate the model using BinaryClassificationEvaluator
evaluator = BinaryClassificationEvaluator(labelCol="target")
lr_auc = evaluator.evaluate(lr_predictions)

print(f"Logistic Regression AUC: {lr_auc}")

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.functions import col

# Precision and Recall
evaluator_precision = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="weightedPrecision")
evaluator_recall = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="weightedRecall")
evaluator_f1 = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="f1")

precision = evaluator_precision.evaluate(lr_predictions)
recall = evaluator_recall.evaluate(lr_predictions)
f1_score = evaluator_f1.evaluate(lr_predictions)

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-Score: {f1_score}")

# Confusion Matrix
lr_predictions.groupBy("target", "prediction").count().show()

import matplotlib.pyplot as plt
import seaborn as sns
from pyspark.sql.functions import col

# Create confusion matrix
y_true = lr_predictions.select("target").collect()
y_pred = lr_predictions.select("prediction").collect()

# Convert to lists for confusion matrix
y_true = [row['target'] for row in y_true]
y_pred = [row['prediction'] for row in y_pred]

# Generate the confusion matrix
from sklearn.metrics import confusion_matrix
cm = confusion_matrix(y_true, y_pred)

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])
plt.title("Logistic Regression - Confusion Matrix")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml.functions import vector_to_array
from sklearn.metrics import roc_curve, auc

# Get probabilities and true labels
probabilities = lr_predictions.select("probability").rdd.map(lambda x: x[0][1]).collect()
labels = y_true

# Compute ROC curve and AUC
fpr, tpr, _ = roc_curve(labels, probabilities)
roc_auc = auc(fpr, tpr)

# Plot ROC curve
plt.figure(figsize=(6, 5))
plt.plot(fpr, tpr, color='blue', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
plt.title('Logistic Regression - ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

# Extract coefficients
coefficients = lr_model.coefficients
intercept = lr_model.intercept
print(f"Intercept: {intercept}")
print("Coefficients: ", coefficients)

# Map feature names to coefficients
feature_names = ["age", "sex", "trestbps", "chol", "fbs", "thalach", "exang", "oldpeak", "ca"]  # Add all your feature names here
for name, coef in zip(feature_names, coefficients):
    print(f"{name}: {coef}")

from pyspark.ml.classification import LinearSVC

# Initialize and train the Support Vector Machine model
svm = LinearSVC(featuresCol="features", labelCol="target")
svm_model = svm.fit(train_data)
svm_predictions = svm_model.transform(test_data)

# Evaluate SVM model
svm_auc = evaluator.evaluate(svm_predictions)

print(f"SVM AUC: {svm_auc}")

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.functions import col

# Precision and Recall
evaluator_precision = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="weightedPrecision")
evaluator_recall = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="weightedRecall")
evaluator_f1 = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="f1")

precision = evaluator_precision.evaluate(svm_predictions)
recall = evaluator_recall.evaluate(svm_predictions)
f1_score = evaluator_f1.evaluate(svm_predictions)

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-Score: {f1_score}")

# Create confusion matrix for SVM
y_true_svm = svm_predictions.select("target").collect()
y_pred_svm = svm_predictions.select("prediction").collect()

# Convert to lists for confusion matrix
y_true_svm = [row['target'] for row in y_true_svm]
y_pred_svm = [row['prediction'] for row in y_pred_svm]

# Generate confusion matrix for SVM
cm_svm = confusion_matrix(y_true_svm, y_pred_svm)

# Plot confusion matrix
plt.figure(figsize=(6, 5))
sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])
plt.title("SVM - Confusion Matrix")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

from pyspark.ml.evaluation import BinaryClassificationEvaluator
from sklearn.metrics import roc_curve, auc

# Get rawPrediction (scores for both classes)
raw_predictions = svm_predictions.select("rawPrediction", "target").rdd.map(lambda row: (row['rawPrediction'][1], row['target'])).collect()

# Separate the predicted probabilities (SVM's score for the positive class)
probabilities_svm = [x[0] for x in raw_predictions]  # score for the positive class
y_true_svm = [x[1] for x in raw_predictions]  # true labels

# Compute ROC curve and AUC
fpr_svm, tpr_svm, _ = roc_curve(y_true_svm, probabilities_svm)
roc_auc_svm = auc(fpr_svm, tpr_svm)

# Plot ROC curve for SVM
plt.figure(figsize=(6, 5))
plt.plot(fpr_svm, tpr_svm, color='green', lw=2, label=f'ROC curve (area = {roc_auc_svm:.2f})')
plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
plt.title('SVM - ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

from pyspark.ml.classification import RandomForestClassifier

# Initialize and train the Random Forest model
rf = RandomForestClassifier(featuresCol="features", labelCol="target")
rf_model = rf.fit(train_data)
rf_predictions = rf_model.transform(test_data)

# Evaluate Random Forest model
rf_auc = evaluator.evaluate(rf_predictions)

print(f"Random Forest AUC: {rf_auc}")

from pyspark.ml.evaluation import MulticlassClassificationEvaluator
from pyspark.sql.functions import col

# Precision and Recall
evaluator_precision = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="weightedPrecision")
evaluator_recall = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="weightedRecall")
evaluator_f1 = MulticlassClassificationEvaluator(labelCol="target", predictionCol="prediction", metricName="f1")

precision = evaluator_precision.evaluate(rf_predictions)
recall = evaluator_recall.evaluate(rf_predictions)
f1_score = evaluator_f1.evaluate(rf_predictions)

print(f"Precision: {precision}")
print(f"Recall: {recall}")
print(f"F1-Score: {f1_score}")

# Create confusion matrix for Random Forest
y_true_rf = rf_predictions.select("target").collect()
y_pred_rf = rf_predictions.select("prediction").collect()

# Convert to lists for confusion matrix
y_true_rf = [row['target'] for row in y_true_rf]
y_pred_rf = [row['prediction'] for row in y_pred_rf]

# Generate confusion matrix for Random Forest
cm_rf = confusion_matrix(y_true_rf, y_pred_rf)

# Plot confusion matrix for Random Forest
plt.figure(figsize=(6, 5))
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues', xticklabels=['No Disease', 'Disease'], yticklabels=['No Disease', 'Disease'])
plt.title("Random Forest - Confusion Matrix")
plt.xlabel('Predicted')
plt.ylabel('True')
plt.show()

# Get probabilities and true labels for Random Forest
probabilities_rf = rf_predictions.select("probability").rdd.map(lambda x: x[0][1]).collect()

# Compute ROC curve and AUC for Random Forest
fpr_rf, tpr_rf, _ = roc_curve(y_true_rf, probabilities_rf)
roc_auc_rf = auc(fpr_rf, tpr_rf)

# Plot ROC curve for Random Forest
plt.figure(figsize=(6, 5))
plt.plot(fpr_rf, tpr_rf, color='red', lw=2, label=f'ROC curve (area = {roc_auc_rf:.2f})')
plt.plot([0, 1], [0, 1], color='grey', lw=2, linestyle='--')
plt.title('Random Forest - ROC Curve')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.legend(loc='lower right')
plt.show()

from pyspark.ml.classification import LinearSVC
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Create the LinearSVC model
svm = LinearSVC(featuresCol='scaled_features', labelCol='target')

# Define the evaluator for binary classification
evaluator = BinaryClassificationEvaluator(labelCol='target')

# Define the parameter grid with fewer options
paramGrid_svm = ParamGridBuilder() \
    .addGrid(svm.regParam, [0.1, 0.01]) \
    .addGrid(svm.maxIter, [10, 50]) \
    .build()

# Create the CrossValidator
crossval_svm = CrossValidator(estimator=svm,
                               evaluator=evaluator,
                               estimatorParamMaps=paramGrid_svm,
                               numFolds=3)  # Reduce numFolds for faster testing

# Sample a subset of the data (only 10% for faster testing)
train_data_subset = train_data.sample(withReplacement=False, fraction=0.1, seed=1234)

# Fit the model using cross-validation on the subset
cv_model_svm = crossval_svm.fit(train_data_subset)

# Get the best model from cross-validation
best_model_svm = cv_model_svm.bestModel

# Print the best parameters
print(f"Best C: {best_model_svm._java_obj.getRegParam()}")
print(f"Best maxIter: {best_model_svm._java_obj.getMaxIter()}")

# Make predictions on the test dataset
svm_predictions = best_model_svm.transform(test_data)

# Show a few predictions
svm_predictions.select("target", "prediction").show(5)

from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from pyspark.ml.evaluation import BinaryClassificationEvaluator

# Initialize the model
lr = LogisticRegression(featuresCol="features", labelCol="target")

# Define a grid of hyperparameters to test
paramGrid = (ParamGridBuilder()
             .addGrid(lr.regParam, [0.01, 0.1, 1.0])  # Regularization parameter
             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])  # Elastic Net mixing
             .build())

# Setup cross-validation
crossval = CrossValidator(estimator=lr,
                          estimatorParamMaps=paramGrid,
                          evaluator=BinaryClassificationEvaluator(labelCol="target"),
                          numFolds=5)  # 5-fold cross-validation

# Run cross-validation to find the best model
cv_model = crossval.fit(train_data)

# Extract the best model
best_lr_model = cv_model.bestModel

# Evaluate the best model on test data
lr_predictions = best_lr_model.transform(test_data)
lr_auc = evaluator.evaluate(lr_predictions)
print(f"Best Logistic Regression Model AUC: {lr_auc}")

from pyspark.ml.classification import LinearSVC

# Initialize the SVM model
svm = LinearSVC(featuresCol="features", labelCol="target")

# Define a grid of hyperparameters to test
paramGrid = (ParamGridBuilder()
             .addGrid(svm.maxIter, [10, 50, 100])
             .addGrid(svm.regParam, [0.01, 0.1, 1.0])
             .build())

# Setup cross-validation
crossval = CrossValidator(estimator=svm,
                          estimatorParamMaps=paramGrid,
                          evaluator=BinaryClassificationEvaluator(labelCol="target"),
                          numFolds=5)

# Run cross-validation and find the best model
cv_model = crossval.fit(train_data)

# Extract the best model
best_svm_model = cv_model.bestModel

from pyspark.ml.classification import RandomForestClassifier

# Initialize the Random Forest model
rf = RandomForestClassifier(featuresCol="features", labelCol="target")

# Define a grid of hyperparameters to test
paramGrid = (ParamGridBuilder()
             .addGrid(rf.numTrees, [10, 50, 100])  # Number of trees
             .addGrid(rf.maxDepth, [5, 10, 20])    # Maximum depth of trees
             .build())

# Setup cross-validation
crossval = CrossValidator(estimator=rf,
                          estimatorParamMaps=paramGrid,
                          evaluator=BinaryClassificationEvaluator(labelCol="target"),
                          numFolds=5)

# Run cross-validation and find the best model
cv_model = crossval.fit(train_data)

# Extract the best model
best_rf_model = cv_model.bestModel

# Get feature importances
print("Feature Importances:", best_rf_model.featureImportances)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Collect metrics for each model
metrics = {
    'Model': ['Logistic Regression', 'SVM', 'Random Forest'],
    'Accuracy': [0.89, 0.89, 0.87],
    'Precision': [0.84, 0.83, 0.77],
    'Recall': [0.82, 0.81, 0.75],
    'F1-Score': [0.82, 0.81, 0.75]
}

# Create a DataFrame
df_metrics = pd.DataFrame(metrics)

# Melt the DataFrame for easy plotting with seaborn
df_melted = df_metrics.melt(id_vars='Model', var_name='Metric', value_name='Score')

# Create a grouped barplot
plt.figure(figsize=(10, 6))
sns.barplot(data=df_melted, x='Metric', y='Score', hue='Model', palette='Set2')

# Add titles and labels
plt.title('Model Comparison by Metrics', fontsize=16)
plt.xlabel('Metric', fontsize=14)
plt.ylabel('Score', fontsize=14)
plt.ylim(0, 1.0)
plt.legend(title='Model', loc='upper right')
plt.xticks(fontsize=12)
plt.yticks(fontsize=12)

# Show the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt
import pandas as pd

# List of feature names corresponding to the 13 features
feature_names = ["age", "sex", "trestbps", "chol", "fbs", "thalach", "exang",
                 "oldpeak", "ca", "cp_indexed", "restecg_indexed", "slope_indexed", "thal_indexed"]

# Corrected list of importances corresponding to the 13 features
importances = [0.05194509674222526, 0.03492655107431265, 0.02941308219342039, 0.024616111339621393,
               0.0010321332095381331, 0.08393308529219531, 0.029130007062087952, 0.058951721902820865,
               0.13396336164513917, 0.141894271065536, 0.00927371885585106, 0.027474143230075136,
               0.13425893970435743]

# Create a DataFrame for sorting and visualization
df_importances = pd.DataFrame({
    'Feature': feature_names,
    'Importance': importances
}).sort_values(by='Importance', ascending=False)

# Plot feature importances
plt.figure(figsize=(10, 6))
plt.barh(df_importances['Feature'], df_importances['Importance'], color='skyblue')
plt.xlabel('Importance', fontsize=14)
plt.ylabel('Feature', fontsize=14)
plt.title('Feature Importances', fontsize=16)
plt.gca().invert_yaxis()  # Invert y-axis for better readability
plt.tight_layout()

# Display the plot
plt.show()

